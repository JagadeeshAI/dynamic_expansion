Using device: cuda
Loading model: meta-llama/Llama-3.2-1B
✓ Froze input embedding layer (model.model.embed_tokens)
✓ Froze output LM head (model.lm_head)
✓ Froze all attention and norm layers
✓ Kept FFN (mlp) layers trainable

============================================================
Model: meta-llama/Llama-3.2-1B
Total Parameters: 1,235,814,400
Trainable Parameters: 805,308,416
Trainable %: 65.16%
Reduction: 34.84% of parameters frozen
============================================================

Loading checkpoint from checkpoints/checkpoint_epoch_3.pt...
Resumed from epoch 3
Previous best exact match: 0
Previously learned samples: 22

Total samples in dataset: 100
One epoch = training on [0], [0,1], [0,1,2], ..., [0,1,...,99]


============================================================
EPOCH 4/10
============================================================

Curriculum direction: backward (N→1)

